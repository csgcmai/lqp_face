""" This file contain functions to read LFW dataset format and generates training and test data for folds"""
from struct import *
import numpy as np
import cPickle as cp
import time
from scipy.io import loadmat
from scipy.io import savemat
import os
import sys
import PCA as PCAAlgo
from PIL import Image 
# from twisted.words.test.test_msn import printError

def get_imlist(path):
    """Get list of images (supports either .jpg or .png images) available at a given path """
    imlist = []
    for filename in os.listdir(path):
        if os.path.splitext(filename)[1] == '.jpg' or os.path.splitext(filename)[1] == '.png':
            imlist.append(os.path.join(path, filename))
    return imlist

def crop_images_with_offsets(ipath, width, height, opath, padding, xoffset, yoffset, dataset=''):
    """ crop images available at the given path 
        ipath = input which contains the set of images to be cropped
        width = width of cropped image.
        height = height of cropped image.
        opath = path where cropped images will be saved...
        padding= number of pixels to add on each side of cropped image
        dataset= if provided perform specific preprocessing for a given dataset 
    """     
    assert(width > 0 and height > 0 and  padding >= 0)

    opath = opath + os.path.sep + "lfw-list-xoffset=" + str(xoffset) + "-yoffset=" + str(yoffset)    
    if not os.path.exists(opath):
        print "Making directory ", opath
        os.mkdir(opath);
        
    if ipath == opath:
        raise ('Error : Input Path: (' + ipath + ') == Output Path (' + opath + ')')
            
    imlist = get_imlist(ipath)
    lfname = os.path.normpath(os.path.join(opath, os.path.pardir 
                                           + os.path.sep)) + os.path.sep + "lfw-list-xoffset=" + str(xoffset) + "-yoffset=" + str(yoffset) + ".txt"
    print lfname
    
    ofile = open(lfname, 'w');
#    return 
    for image in imlist:
        im = Image.open(image)        
        if im.mode == 'RGB':
            im.convert('L');  # convert to grayscale
        if dataset == 'FERET':
            # Rescale the image... 
            print ' Rescaling the images for FERET Dataset'
            im.resize((im[0] * 0.7, im[1] * 0.7));
        im = np.array(im);
        x = np.floor((im.shape[1] - (width + 2 * padding)) / 2) - 1;
        y = np.floor ((im.shape[0] - (height + 2 * padding)) / 2) - 1;
        
        x = x + xoffset
        y = y + yoffset;
        print 'x = %d, y=%d' % (x, y)
        cr_image = Image.fromarray(im [y:y + height + 2 * padding, x:x + width + 2 * padding ]);
        imname = os.path.join(opath, os.path.basename(image).rsplit('.')[0] + '.jpg')        
        cr_image.save(imname)
        ofile.write(imname + "\n")
    ofile.close()

def crop_images(ipath, width, height, opath, padding=0, dataset=''):
    """ crop images available at the given path 
        ipath = input which contains the set of images to be cropped
        width = width of cropped image.
        height = height of cropped image.
        opath = path where cropped images will be saved...
        padding= number of pixels to add on each side of cropped image
        dataset= if provided perform specific preprocessing for a given dataset 
    """     
    assert(width > 0 and height > 0 and  padding >= 0)
    
    if not os.path.exists(opath):
        os.mkdir(opath);
        
    if ipath == opath:
        raise ('Error : Input Path: (' + ipath + ') == Output Path (' + opath + ')')
            
    imlist = get_imlist(ipath)
    
    for image in imlist:
        im = Image.open(image)        
        if im.mode == 'RGB':
            im.convert('L');  # convert to grayscale
        if dataset == 'FERET':
            # Rescale the image... 
            print ' Rescaling the images for FERET Dataset'
            im.resize((im[0] * 0.7, im[1] * 0.7));
        im = np.array(im);
        x = np.floor((im.shape[1] - (width + 2 * padding)) / 2) - 1;
        y = np.floor ((im.shape[0] - (height + 2 * padding)) / 2) - 1;
        
        x = x + xoffset
        y = y + yoffset;
        print 'x = %d, y=%d' % (x, y)
        cr_image = Image.fromarray(im [y:y + height + 2 * padding, x:x + width + 2 * padding ]);        
        cr_image.save(os.path.join(opath, os.path.basename(image).rsplit('.')[0] + '.jpg'))
        
def mean_image(dir):
    """ Computes Mean of the images present at the given path """ 
    images = get_imlist(dir);
    im = np.zeros(np.array(Image.open(images[0])).shape);
    for image in images:
        im = im + np.array(Image.open(image));  
    im = im / len(images);
    return im;
 
def train_pca(data, ncomp):
    """ 
        Train Whitened PCA on the given data
        data = m x n numpy array 
            where m is number of examples, n number of dimensions of each example
        ncomp = number of components to retain.  
    """
    print " Learning PCA ...."
    st = time.time()
    ncomp = np.min((np.min(data.shape), ncomp))
    use_exterior = False
    if data.shape[0] < data.shape[1]:
        use_exterior = True  # use exterior product for fast computation of PCA, see PCA.py for details
    
    PCA = PCAAlgo.PCA(ncomp, extern=use_exterior);
    PCA.fit(data);
    pdata = PCA.transform(data, whiten=True);
    et = time.time() - st;
    print " Time Taken by Learning PCA = %0.1f sec" % et
    return PCA, pdata

def compute_distance(lfeat, rfeat, dtype, flfeat, frfeat):
    """ 
        compute distance between given matrices for pair of images i.e. lfeat and rfeat 
        lfeat = m x n matrix for left item of the pair, where m is number of examples and n is number of features...
        if flipped features are given then minimum between among the four possible options
        i.e min( min(lfeat,rfeat) , min(lfeat,frfeat), min(flfeat,rfeat), min(flfeat,frfeat) )
        is returned
        dtype = type of distance metric 
        """
    if dtype.lower() == "cosine":
        norm1 = np.sqrt (np.sum (np.power(lfeat, 2), 1));
        norm2 = np.sqrt (np.sum (np.power(rfeat, 2), 1));
        dis1 = (np.sum (lfeat * rfeat, 1) / (norm1 * norm2));
    
        norm3 = np.sqrt (np.sum (np.power(frfeat, 2), 1));            
        dis2 = np.sum (lfeat * frfeat, 1) / (norm1 * norm3);
        
        norm4 = np.sqrt (np.sum (np.power(flfeat, 2), 1));
        dis3 = np.sum (rfeat * flfeat, 1) / (norm2 * norm4);
        
        dis4 = np.sum (flfeat * frfeat, 1) / (norm3 * norm4);
        
        dist = -np.maximum(np.maximum(np.maximum(dis1, dis2), dis3), dis4);
        
    elif dtype.lower() == 'l2':
        dist = np.sum((np.power(lfeat - rfeat), 2), 1);
    else:
        raise " Error: Invalid option for computing distance"
    return dist
        
def find_optimal_threshold(distance, labels, method):
    """
        Find the optimal threshold for the decision process
        distance = computed distances betweeen image pairs
        labels = class labels
        method = method used for finding the threshold 
    """
    npos = np.sum(labels == 1);  # number of positive examples;
    sdist = np.sort(distance);
    if method.upper() == "EER":
         threshold = sdist[npos];  # number of -ve's =  number of +ve's
    else:
        raise " Error: Invalid method type for the configuration of Threhsold"
    return threshold

def read_LFW(fname): 
    """ read LFW dataset annotation information with names and labels... """
#    dir = os.getcwd()
# os.chdir(dirname)
    if not os.path.exists(fname):
        raise 'LFW File :', fname, 'does not exist'
    lines = open(fname).readlines();
    print lines
    for l in lines:
        print l

def read_feature_file(fname):
    """
        Reads the feature from the given fname:
    """
    fin = open(fname, 'rb');
    buffer = fin.read();
    tvar = list(unpack('3i', buffer[0:12]));
    nrows = tvar[1];   
    colsize = tvar[2];
    features = np.array(unpack(str(nrows * colsize) + 'f', buffer[12:]));
    assert(nrows * colsize == features.size)
    fin.close()
    return features, nrows, colsize

def read_descriptors(inames, featdir, ftype, dstr=" ------ Descriptors ---------"):
    """
     Read Descriptors for the given list of images and return 
    function [litem,ritem,flitem,fritem,nd]=readDescriptors(inames,featdir,ftype,dstr,pprocess,nd)
    read the descriptors written by mfiles
    iname: name of images
    featdir:feature directory containing computed features.
    ftype: [LBP, LTP, LQP]
    dstr: display string 
    Output=
    litem: features for image pair left item;
    ritem: features for image pair right item;
    litem: features for flipped-image pair left item;
    ritem: features for flipped-image pair right item;
    """
#    cellsize = 10;
#    cbsize = 300;  # 2 x 150    
    ftype = ftype.upper();
    nimages = len(inames);
    feat, m, n = read_feature_file(os.path.join(featdir, inames[0][0][0] + '.jpg-' + ftype));
#    n = n / cbsize; ncells = m * n;
        
    litem = np.zeros((nimages, feat.size), np.float32);
    flitem = np.zeros((nimages, feat.size), np.float32);
    ritem = np.zeros((nimages, feat.size), np.float32);
    fritem = np.zeros((nimages, feat.size), np.float32);
    st = time.time();
    
    print 'Starting Reading Descriptors for %s' % dstr;

    for k, images in enumerate(inames):
        lename = os.path.join(featdir, inames[k][0][0] + '.jpg-' + ftype);
        rename = os.path.join(featdir, inames[k][1][0] + '.jpg-' + ftype);
        
        litem[k, :] = read_feature_file(lename)[0];
        ritem[k, :] = read_feature_file(rename)[0];
        
        flitem[k, :] = read_feature_file(lename + '-f')[0];
        fritem[k, :] = read_feature_file(rename + '-f')[0];    
    
        if k % 200 == 0 :
            print ' Reading %d/%d' % (k, len(inames)) 
    et = time.time() - st;
    
    print 'Finished Reading Descriptors for %s \n Total Time Taken by Reading Operation =%f\n' % (dstr, et);
    return litem, ritem, flitem, fritem
    
def read_compute_distance(PCA, images, featdir, ftype, dtype):
    """
        Read features and then Project them to learn PCA components
        and finally compute distance between the projected descriptors...        
        PCA = PCA class used to project features...
        images = name of images to read
        featdir= feature directory
        ftype = [LBP, LTP, LQP]
        dtype = distance type [cosine, L2]
        
        output:
        dist= computed distance between pair of projected descriptors...
    """
    litem, ritem, flitem, fritem = read_descriptors(images, featdir, ftype)
        # project the features
    litem = PCA.transform(litem, whiten=True);
    ritem = PCA.transform(ritem, whiten=True);
    flitem = PCA.transform(flitem, whiten=True);
    fritem = PCA.transform(fritem, whiten=True);
    # find distance between pair of iamges...
    dist = compute_distance(litem, ritem, dtype, flitem, fritem)
    
    return dist
def compute_accuracy(dist, labels, threshold):
    """
         Compute the average accuracy over the given set of images.
         dist: computed distance between pair of images.
         labels: true class labels
         threshold: decision threshold.
    """
    trueclass = np.sum(np.logical_and((dist <= threshold).reshape(1000, 1), labels == 1)
           ) + np.sum(np.logical_and((dist > threshold).reshape(1000, 1), labels != 1)) 
    accuracy = float(trueclass) / len(labels)
    return accuracy
def train_test(data, featdir, ftype, pcadim, filesuffix):
    """
    Train the PCA and Evaluate on the test set..
    data = list of images name for training and testing.
    ftype = feature type
    pcadim= number of PCA-dimensions...
    filesuffix= used for storing temporary data...
    """
    pcaTrain, pcalabel, thTrain, thlabel, test, testlabels = data
     
    if isinstance(pcadim, list):
        ncomp = max(pcadim)
    else:
        ncomp = pcadim
     # read features for the PCA learning...
     
    pcafile = "./data/PCA-" + str(ncomp) + "-" + filesuffix + ".npy"
    if not os.path.exists(pcafile): 
        litem, ritem, flitem, fritem = read_descriptors(pcaTrain, featdir, ftype)
        pcafeat = np.concatenate((litem, ritem))
        PCA, pfeat = train_pca(pcafeat, ncomp)
        np.save(pcafile, [PCA, pfeat]);
    else:
        PCA, pfeat = np.load(pcafile)
    
    count = 1;
    accuracy = []
    for k in pcadim:
        suffix = filesuffix + "-ncomp=" + str(k)
        # read and project features and compute the distance between pair of images...
        dist = read_compute_distance(PCA, thTrain, featdir, ftype, "cosine")
        threshold = find_optimal_threshold(dist, thlabel, "EER")
        dist = read_compute_distance(PCA, test, featdir, ftype, "cosine")
        accuracy.append(compute_accuracy(dist, testlabels, threshold))
        print "For PCA Dim = %d, Classification accuracy(on view = %s)= %f" % (k, filesuffix, accuracy[-1])
    return PCA, accuracy         
     

def LFW(view, featdir, ftype='LQP', chunk=1800):
    """
        Read the features for the given view and train & evaluate the PCA model for the given set
        of examples:
        view= data from view1 or view2 to use (see LFW documentation)
        featdir= directory path of features.  
        ftype =  any of [LBP, LTP, LQP] features to use
        chunk =  used only during the cross-validation to find the best number of PCA components. 
     """
    view = view.lower()
    ftype = ftype.upper()
    ncomp = 2000;  # number of PCA components
    
    if view not in ["view1", "view2"]:
        raise "Error: Provide the Dataset view (either view1 or view2)to use for the training "
    if ftype not in ["LBP", "LTP", "LQP"]:
        raise "Error: Wrong Feature Type, choose any one of the following features [LBP, LTP, LQP] "
   
    pcadim = range(chunk, ncomp, chunk)
    pcafile = "./data/" + view + "-PCA-" + ftype + ".npy"    
   
    if os.path.exists(pcafile):
        PCA, accuracy = load(pcafile)
    else:
        data = np.load(view + '.npy')
        if view == "view1":
            PCA, accuracy = train_test(data, featdir, ftype, pcadim, 'view1')
            np.save(pcafile, [PCA, accuracy])
        else:
            accuracy = []
            PCA = []
            for k in range(10):  # 10 fold
                # extract training data
                tdata = []    
                for t in range(6):
                    tdata.append(data[t][0][k])
 
                #                  
                ipca, iaccuracy = train_test(data, featdir, ftype, pcadim, 'view2')
                accuracy.append(iaccuracy)
                PCA.append(ipca)
            
            np.save(pcafile, [PCA, accuracy])

if __name__ == "__main__":
#    crop_images('/home/hussain/dataset/', 80 , 150, '/tmp/', 10, 'LFW');
        LFW('view1', '/home/hussain/research/code-release/new-data-xoffset=1-yoffset=-4/lbp-norm-1/', 'LBP', 500)
